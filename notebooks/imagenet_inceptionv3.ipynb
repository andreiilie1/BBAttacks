{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os, time\n",
    "# Select GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
    "# from tensorflow.keras.applications.mobilenet import preprocess_input, MobileNet\n",
    "# from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_accuracy(y_true, y_pred, k=1):\n",
    "    '''From: https://github.com/chainer/chainer/issues/606\n",
    "    \n",
    "    Expects both y_true and y_pred to be one-hot encoded.\n",
    "    '''\n",
    "    argsorted_y = np.argsort(y_pred)[:,-k:]\n",
    "    return np.any(argsorted_y.T == y_true.argmax(axis=1), axis=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define data paths and load data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/ailie/Repos/BBAttacks/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = os.path.join(BASE_PATH, \"x_val_0_10000.npy\")\n",
    "LABELS_PATH = os.path.join(BASE_PATH, \"y_val.npy\")\n",
    "SYNSET_WORDS_PATH = os.path.join(BASE_PATH, \"synset_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_idx_to_name = {}\n",
    "f = open(SYNSET_WORDS_PATH,\"r\")\n",
    "idx = 0\n",
    "for line in f:\n",
    "    parts = line.split(\" \")\n",
    "    keras_idx_to_name[idx] = \" \".join(parts[1:])\n",
    "    idx += 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_raw = np.load(IMAGES_PATH) # loaded as RGB\n",
    "# didn't upload imagenet data here as it is too large, but it is publicl available\n",
    "# and the experiments are reproducible \n",
    "x_val = preprocess_input(x_val_raw.copy()) # converted to BGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.load(LABELS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load model and only keep a small batch of correctly classified images </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep a few images in memory and drop the others\n",
    "TOTAL_SAMPLE_SIZE = 5000\n",
    "x_val = x_val[:TOTAL_SAMPLE_SIZE]\n",
    "x_val_raw = x_val_raw[:TOTAL_SAMPLE_SIZE]\n",
    "\n",
    "y_val = y_val[:TOTAL_SAMPLE_SIZE]\n",
    "y_val_one_hot = to_categorical(y_val, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 13:06:52.902536: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2022-10-05 13:06:54.556558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:04:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-10-05 13:06:54.556881: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-10-05 13:06:54.559400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-10-05 13:06:54.561513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2022-10-05 13:06:54.561878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2022-10-05 13:06:54.564296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-10-05 13:06:54.565639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-10-05 13:06:54.570533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-10-05 13:06:54.570990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2022-10-05 13:06:54.571779: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2022-10-05 13:06:54.599389: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2997990000 Hz\n",
      "2022-10-05 13:06:54.601736: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556b9508e560 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-10-05 13:06:54.601753: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-10-05 13:06:54.602052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:04:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-10-05 13:06:54.602085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-10-05 13:06:54.602106: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-10-05 13:06:54.602123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2022-10-05 13:06:54.602140: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2022-10-05 13:06:54.602157: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-10-05 13:06:54.602174: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-10-05 13:06:54.602191: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-10-05 13:06:54.602533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2022-10-05 13:06:54.602582: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-10-05 13:06:54.773548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-10-05 13:06:54.773573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2022-10-05 13:06:54.773578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2022-10-05 13:06:54.774188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10378 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\n",
      "2022-10-05 13:06:54.776067: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556b98832f00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-10-05 13:06:54.776081: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = InceptionV3(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 299, 299, 3) for input Tensor(\"input_1:0\", shape=(None, 299, 299, 3), dtype=float32), but it was called on an input with incompatible shape (None, 224, 224, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 13:07:01.849003: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-10-05 13:07:01.996825: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 7s 42ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.7146\n",
      "Top-3 accuracy: 0.8612\n",
      "Top-5 accuracy: 0.8976\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 3, 5]:\n",
    "    top_k_acc = top_k_accuracy(y_val_one_hot, y_pred, k)\n",
    "    print(f\"Top-{k} accuracy: {top_k_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of correctly classified images\n",
    "correct_indices = []\n",
    "agreements = np.argmax(y_pred, axis=1) == np.argmax(y_val_one_hot, axis=1)\n",
    "for x in range(len(agreements)):\n",
    "    if agreements[x]:\n",
    "        correct_indices.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SAMPLE_SIZE = 50\n",
    "RANDOM_SEED = 1337\n",
    "\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "sample_correct_indices = random.choices(correct_indices, k=RANDOM_SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(x_val_raw[458]/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_indices = range(500)\n",
    "y_val_one_hot_sample = y_val_one_hot[sample_correct_indices]\n",
    "x_val_sample = x_val[sample_correct_indices]\n",
    "x_val_raw_sample = x_val_raw[sample_correct_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 178ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_sample = model.predict(x_val_sample, verbose=1)\n",
    "assert top_k_accuracy(y_val_one_hot_sample, y_pred_sample, k=1) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there are enough correctly classified images\n",
    "assert len(x_val_sample) == RANDOM_SAMPLE_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Import attack utils </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, Dense, Flatten, BatchNormalization\n",
    "import importlib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ailie/Repos/BBAttacks/attacks/\")\n",
    "sys.path.append(\"/home/ailie/Repos/BBAttacks/utils/\")\n",
    "\n",
    "import utils\n",
    "from data_manager import load_data\n",
    "\n",
    "# Black Box Attacks\n",
    "import random_noise\n",
    "import EvoStrategyUniformProbs\n",
    "import SimbaWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evolutionary attack</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoba_params = {\n",
    "    \"gen_size\": 3,\n",
    "    \"px_count\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [10:20<00:00, 12.40s/it]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(EvoStrategyUniformProbs)\n",
    "perturbed_images = 0\n",
    "adv_evo_strategy = {}\n",
    "failed_indices = []\n",
    "\n",
    "for index in tqdm(range(RANDOM_SAMPLE_SIZE)):\n",
    "    img = x_val_raw_sample[index]\n",
    "    label = np.argmax(y_val_one_hot_sample[index])\n",
    "    \n",
    "    adv_evo_strategy[index] = EvoStrategyUniformProbs.AdversarialPerturbationEvoStraegy(\n",
    "        model=model,\n",
    "        img=img,\n",
    "        label=label,\n",
    "        generation_size=evoba_params[\"gen_size\"], \n",
    "        one_step_perturbation_pixel_count=evoba_params[\"px_count\"],\n",
    "        verbose=False,\n",
    "        zero_one_scale=False,\n",
    "        range_scale_int=True,\n",
    "        preprocess = preprocess_input\n",
    "    )\n",
    "    \n",
    "    no_steps = adv_evo_strategy[index].run_adversarial_attack(steps=10000)\n",
    "    if adv_evo_strategy[index].is_perturbed() and no_steps > 0:\n",
    "        perturbed_images += 1\n",
    "    \n",
    "    if not adv_evo_strategy[index].is_perturbed():\n",
    "        failed_indices.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '/home/ailie/Repos/BBAttacks/mlruns/mlruns/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ailie/anaconda3/envs/tf_env/lib/python3.7/site-packages/mlflow/store/tracking/file_store.py\", line 261, in list_experiments\n",
      "    experiment = self._get_experiment(exp_id, view_type)\n",
      "  File \"/home/ailie/anaconda3/envs/tf_env/lib/python3.7/site-packages/mlflow/store/tracking/file_store.py\", line 344, in _get_experiment\n",
      "    meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/ailie/anaconda3/envs/tf_env/lib/python3.7/site-packages/mlflow/utils/file_utils.py\", line 175, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/ailie/Repos/BBAttacks/mlruns/mlruns/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended previous run\n",
      "Logging run EvoBA under experiment IMAGENET\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.generate_mlflow_logs(\n",
    "    strategy_objects=adv_evo_strategy, \n",
    "    attack_type=utils.AttackType.EVOBA, \n",
    "    unperturbed_images=x_val_raw_sample, \n",
    "    run_name=\"EvoBA\", \n",
    "    experiment_name=\"IMAGENET\",\n",
    "    additional_params=evoba_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoba_stats = utils.get_evoba_stats(adv_evo_strategy)\n",
    "utils.print_evoba_stats(evoba_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(EvoStrategy)\n",
    "# # perturbed_images = 0\n",
    "# # adv_evo_strategy = {}\n",
    "# for index in range(50,100):\n",
    "#     print()\n",
    "#     print(index)\n",
    "#     img = x_val_raw_sample[index]\n",
    "# #     print(img)\n",
    "#     label = np.argmax(y_val_one_hot_sample[index])\n",
    "#     adv_evo_strategy[index] = EvoStrategy.AdversarialPerturbationEvoStraegy(\n",
    "#         model=model,\n",
    "#         img=img,\n",
    "#         label=label,\n",
    "#         generation_size=20, \n",
    "#         one_step_perturbation_pixel_count=1,\n",
    "#         verbose=True,\n",
    "#         zero_one_scale=False,\n",
    "#         range_scale_int=True,\n",
    "# #         max_rand=int(x_val.max()),\n",
    "# #         min_rand=int(x_val.min())\n",
    "#         preprocess = preprocess_input\n",
    "#     )\n",
    "#     no_steps = adv_evo_strategy[index].run_adversarial_attack(steps=1000)\n",
    "#     if adv_evo_strategy[index].stop_criterion() and no_steps > 0:\n",
    "#         perturbed_images += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AdversarialPerturbationEvoStraegy' object has no attribute 'stop_criterion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_596541/2015355806.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madv_evo_strategy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AdversarialPerturbationEvoStraegy' object has no attribute 'stop_criterion'"
     ]
    }
   ],
   "source": [
    "for index in range(50):\n",
    "    if not adv_evo_strategy[index].stop_criterion():\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_q = 0\n",
    "sum_q = 0\n",
    "query_list = []\n",
    "fair_indices = []\n",
    "for index in range(50):\n",
    "    if(adv_evo_strategy[index].queries > 1):\n",
    "        count_q +=1\n",
    "        sum_q += adv_evo_strategy[index].queries\n",
    "        query_list.append(adv_evo_strategy[index].queries)\n",
    "        fair_indices.append(index)\n",
    "        \n",
    "print(\"Average queries for EVO algo:\", sum_q/count_q)\n",
    "print(\"Total count of perturbed images (classified correctly initially):\", perturbed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Query histogram for EvoBA(1, 15) on ImageNet\", fontdict={\"size\":22})\n",
    "plt.hist(query_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "l0_dists = []\n",
    "for index_diff in tqdm(fair_indices):\n",
    "    diff = np.abs(adv_evo_strategy[index_diff].get_best_candidate() - x_val_raw_sample[index_diff])\n",
    "#     diff = np.reshape(diff, (32, 32, 3))\n",
    "    diff = (diff!=0)\n",
    "    l0_dist = np.sum(diff)\n",
    "    l0_dists.append(l0_dist)\n",
    "#     print(\"L2 distance:\", math.sqrt(np.sum(np.reshape(diff, (-1))**2)))\n",
    "#     plt.imshow(np.reshape(adv_evo_strategy[index_diff].get_best_candidate(), (28, 28)))\n",
    "#     plt.show()\n",
    "#     print(\"Prediction:\", model.predict(np.array([adv_evo_strategy[index_diff].get_best_candidate()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(l0_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(l2_dists)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"L0 histogram for EvoBA(1, 15) on ImageNet\", fontdict={\"size\":22})\n",
    "plt.hist(l0_dists)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "l2_dists = []\n",
    "for index_diff in tqdm(fair_indices):\n",
    "    diff = np.abs(adv_evo_strategy[index_diff].get_best_candidate() - x_val_raw_sample[index_diff])\n",
    "#     diff = np.reshape(diff, (32, 32, 3))\n",
    "    l2_dist = math.sqrt(np.sum(np.reshape(diff, (-1))**2))\n",
    "    l2_dists.append(l2_dist)\n",
    "#     print(\"L2 distance:\", math.sqrt(np.sum(np.reshape(diff, (-1))**2)))\n",
    "#     plt.imshow(np.reshape(adv_evo_strategy[index_diff].get_best_candidate(), (28, 28)))\n",
    "#     plt.show()\n",
    "#     print(\"Prediction:\", model.predict(np.array([adv_evo_strategy[index_diff].get_best_candidate()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x_val_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(l2_dists)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(l2_dists)/(255 * 224 * 224 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=10\n",
    "h=10\n",
    "fig=plt.figure(figsize=(20, 9))\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.1, wspace=0.2)\n",
    "\n",
    "columns = 5\n",
    "rows = 2\n",
    "img_curr = 0\n",
    "for i in range(1, columns + 1):\n",
    "    img_indx = fair_indices[imgss[img_curr]]\n",
    "    initial = (keras_idx_to_name[adv_evo_strategy[img_indx].label])\n",
    "    if len(initial) > 30:\n",
    "        initial = initial[:21] + \"\\n\" + initial[21:]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.title(f\"Original, \\n {initial}\", fontdict={\"size\":18})\n",
    "    img_start = adv_evo_strategy[img_indx].img.astype(int)\n",
    "    plt.imshow(img_start)\n",
    "    \n",
    "    fig.add_subplot(rows, columns, i + 5)\n",
    "    \n",
    "    img_final = adv_evo_strategy[img_indx].get_best_candidate().astype(int)\n",
    "    predss = model.predict(np.expand_dims(img_final, axis=0))[0]\n",
    "    predicted = np.argmax(predss)\n",
    "    diff = math.sqrt(np.sum((img_final - img_start) **2))\n",
    "    l2_distance = int((diff/(255)) * 100)/100\n",
    "    l0_distance = (img_start != img_final).sum()\n",
    "    final = (keras_idx_to_name[predicted])\n",
    "    plt.title(f\"Perturbed, \\n {final} L2 distance:{l2_distance}\\n L0 distance:{l0_distance}\", fontdict={\"size\":18})\n",
    "    plt.imshow(img_final)\n",
    "    img_curr += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_diff in tqdm(np.array(fair_indices)[imgss]):\n",
    "    print(index_diff)\n",
    "    initial = (keras_idx_to_name[adv_evo_strategy[index_diff].label])\n",
    "    img = adv_evo_strategy[index_diff].get_best_candidate().astype(int)\n",
    "    plt.imshow(img.astype(int))\n",
    "    predicted = (np.argmax(model.predict(np.expand_dims(img, axis=0))[0]))\n",
    "    final = (keras_idx_to_name[predicted])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(EvoStrategy)\n",
    "from tqdm import tqdm\n",
    "perturbed_images_bf = 0\n",
    "adv_evo_strategy_bf = {}\n",
    "for index in tqdm(range(len(x_val_sample))):\n",
    "    if index % 10 == 0:\n",
    "        verbose = True\n",
    "    else:\n",
    "        verbose = False\n",
    "    img = x_val_sample[index]\n",
    "    label = np.argmax(y_val_one_hot_sample[index])\n",
    "    adv_evo_strategy_bf[index] = EvoStrategy.AdversarialPerturbationBFStraegy(\n",
    "        model=model,\n",
    "        img=img,\n",
    "        label=label,\n",
    "        generation_size=20, \n",
    "        one_step_perturbation_pixel_count=5,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    no_steps_bf = adv_evo_strategy_bf[index].run_adversarial_attack(steps=100)\n",
    "    if adv_evo_strategy_bf[index].stop_criterion() and no_steps_bf > 0:\n",
    "        perturbed_images_bf += 1\n",
    "    adv_evo_strategy_bf[index].active_generation = []\n",
    "    adv_evo_strategy_bf[index].fitness_scores = []\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_q_bf = 0\n",
    "sum_q_bf = 0\n",
    "for index in tqdm(range(len(x_val_sample))):\n",
    "    if(adv_evo_strategy_bf[index].queries > 1):\n",
    "        count_q_bf +=1\n",
    "        sum_q_bf += adv_evo_strategy_bf[index].queries\n",
    "        \n",
    "print(\"Average queries for EVO algo:\", sum_q_bf/count_q_bf)\n",
    "print(\"Total count of perturbed images (classified correctly initially):\", perturbed_images_bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "671/len(x_val_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "l2_dists = []\n",
    "for index_diff in tqdm(range(50)):\n",
    "    diff = np.abs(adv_evo_strategy[index_diff].get_best_candidate() - x_val_sample[index_diff])\n",
    "#     diff = np.reshape(diff, (32, 32, 3))\n",
    "    l2_dist = math.sqrt(np.sum(np.reshape(diff, (-1))**2))\n",
    "    l2_dists.append(l2_dist)\n",
    "#     print(\"L2 distance:\", math.sqrt(np.sum(np.reshape(diff, (-1))**2)))\n",
    "#     plt.imshow(np.reshape(adv_evo_strategy[index_diff].get_best_candidate(), (28, 28)))\n",
    "#     plt.show()\n",
    "#     print(\"Prediction:\", model.predict(np.array([adv_evo_strategy[index_diff].get_best_candidate()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x_val_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(l2_dists)/(255 * 224 * 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(EvoStrategy)\n",
    "# TODO: add per channel perturbation, verify that it succeeds faster\n",
    "# TODO: momentum approach\n",
    "perturbed_images = 0\n",
    "adv_evo_strategy = {}\n",
    "for index in range(100):\n",
    "    print()\n",
    "    print(index)\n",
    "    img = x_val_sample[index]\n",
    "    label = np.argmax(y_val_one_hot_sample[index])\n",
    "    adv_evo_strategy[index] = EvoStrategy.AdversarialPerturbationEvoStraegy(\n",
    "        model=model,\n",
    "        img=img,\n",
    "        label=label,\n",
    "        generation_size=20, \n",
    "        one_step_perturbation_pixel_count=10\n",
    "    )\n",
    "    no_steps = adv_evo_strategy[index].run_adversarial_attack(steps=50)\n",
    "    if adv_evo_strategy[index].stop_criterion() and no_steps > 0:\n",
    "        perturbed_images += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_q = 0\n",
    "sum_q = 0\n",
    "for index in range(0,100):\n",
    "    if(adv_evo_strategy[index].queries > 1):\n",
    "        count_q +=1\n",
    "        sum_q += adv_evo_strategy[index].queries\n",
    "        \n",
    "print(\"Average queries for EVO algo:\", sum_q/count_q)\n",
    "print(\"Total count of perturbed images (classified correctly initially):\", perturbed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(EvoStrategy)\n",
    "import gc\n",
    "# TODO: add per channel perturbation, verify that it succeeds faster\n",
    "# TODO: momentum approach\n",
    "perturbed_images = 0\n",
    "adv_evo_strategy = {}\n",
    "for index in range(100):\n",
    "    print()\n",
    "    print(index)\n",
    "    img = x_val_sample[index]\n",
    "    label = np.argmax(y_val_one_hot_sample[index])\n",
    "    adv_evo_strategy[index] = EvoStrategy.AdversarialPerturbationEvoStraegy(\n",
    "        model=model,\n",
    "        img=img,\n",
    "        label=label,\n",
    "        generation_size=50, \n",
    "        one_step_perturbation_pixel_count=10\n",
    "    )\n",
    "    no_steps = adv_evo_strategy[index].run_adversarial_attack(steps=40)\n",
    "    if adv_evo_strategy[index].stop_criterion() and no_steps > 0:\n",
    "        perturbed_images += 1\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_q = 0\n",
    "sum_q = 0\n",
    "for index in range(0,100):\n",
    "    if(adv_evo_strategy[index].queries > 1):\n",
    "        count_q +=1\n",
    "        sum_q += adv_evo_strategy[index].queries\n",
    "        \n",
    "print(\"Average queries for EVO algo:\", sum_q/count_q)\n",
    "print(\"Total count of perturbed images (classified correctly initially):\", perturbed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(SimbaWrapper)\n",
    "simba_wrapper = SimbaWrapper.SimbaWrapper(model, x_val_sample, y_val_one_hot_sample, 0.1, max_queries=2000, max_l0_distance=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simba_wrapper.run_simba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simba_wrapper.X_modified[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_sample[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
